{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLNm0dTQ5rcj/EFrInSDk0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariamamgad8/HumanAI_GSoC_Proposal_2025/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **retrieving and storing filtered social media posts**"
      ],
      "metadata": {
        "id": "maDI-fekjtww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Twitter API credentials (replace with your own)\n",
        "BEARER_TOKEN = \"your API\"\n",
        "\n",
        "# Predefined keywords related to mental health\n",
        "KEYWORDS = [\n",
        "    \"depressed\", \"depression\", \"anxious\", \"anxiety\",\n",
        "    \"suicidal\", \"want to die\", \"addiction help\",\n",
        "    \"overwhelmed\", \"mental health\", \"therapy\",\n",
        "    \"self harm\", \"lonely\", \"hopeless\", \"panic attack\",\n",
        "    \"stress\"\n",
        "]\n",
        "\n",
        "# Clean text function\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove user @ references and '#' from hashtags\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "    # Remove emojis and special characters\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Initialize Twitter client\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "def search_tweets(keywords, max_results=100):\n",
        "    # Combine keywords with OR for the query\n",
        "    query = \" OR \".join(keywords) + \" -is:retweet lang:en\"\n",
        "\n",
        "    try:\n",
        "        # Search recent tweets (free tier allows last 7 days only)\n",
        "        tweets = client.search_recent_tweets(\n",
        "            query=query,\n",
        "            tweet_fields=[\"created_at\", \"public_metrics\"],\n",
        "            max_results=max_results\n",
        "        )\n",
        "\n",
        "        return tweets.data if tweets.data else []\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching tweets: {e}\")\n",
        "        return []\n",
        "\n",
        "def process_tweets(tweets):\n",
        "    processed = []\n",
        "    for tweet in tweets:\n",
        "        cleaned_text = clean_text(tweet.text)\n",
        "        metrics = tweet.public_metrics\n",
        "\n",
        "        processed.append({\n",
        "            \"id\": tweet.id,\n",
        "            \"timestamp\": tweet.created_at.isoformat(),\n",
        "            \"content\": tweet.text,\n",
        "            \"cleaned_content\": cleaned_text,\n",
        "            \"likes\": metrics[\"like_count\"],\n",
        "            \"retweets\": metrics[\"retweet_count\"],\n",
        "            \"replies\": metrics[\"reply_count\"],\n",
        "            \"impressions\": metrics[\"impression_count\"] if \"impression_count\" in metrics else None\n",
        "        })\n",
        "\n",
        "    return processed\n",
        "\n",
        "def save_data(data, filename_prefix=\"mental_health_tweets\"):\n",
        "    # Save as JSON\n",
        "    json_filename = f\"{filename_prefix}.json\"\n",
        "    with open(json_filename, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    # Save as CSV\n",
        "    df = pd.DataFrame(data)\n",
        "    csv_filename = f\"{filename_prefix}.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    print(f\"Data saved to {json_filename} and {csv_filename}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Searching for mental health related tweets...\")\n",
        "    tweets = search_tweets(KEYWORDS)\n",
        "\n",
        "    if not tweets:\n",
        "        print(\"No tweets found matching the criteria.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(tweets)} tweets. Processing...\")\n",
        "    processed_tweets = process_tweets(tweets)\n",
        "\n",
        "    # Save the data\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename_prefix = f\"mental_health_tweets_{timestamp}\"\n",
        "    save_data(processed_tweets, filename_prefix)\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omi_x3Xs_FFK",
        "outputId": "a01ddeb5-c55f-41c0-8179-e523157a6254"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for mental health related tweets...\n",
            "Found 100 tweets. Processing...\n",
            "Data saved to mental_health_tweets_20250406_091537.json and mental_health_tweets_20250406_091537.csv\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **cleaning dataset to make it ready for NLP analysis**"
      ],
      "metadata": {
        "id": "uWo9h6N4jxr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from better_profanity import profanity\n",
        "\n",
        "# === Setup ===\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "profanity.load_censor_words()\n",
        "\n",
        "# === Load dataset ===\n",
        "df = pd.read_csv('/content/mental_health_tweets.csv') #replace it with the correct location\n",
        "\n",
        "# === Helper: reduce stretched words (e.g., \"soooo\" → \"soo\") ===\n",
        "def reduce_stretch(word):\n",
        "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
        "\n",
        "# === Cleaning Function ===\n",
        "def clean_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "    # Remove emojis\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize and filter\n",
        "    words = text.split()\n",
        "    cleaned_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # Reduce stretched characters\n",
        "        word = reduce_stretch(word)\n",
        "\n",
        "        # Skip stopwords and short gibberish\n",
        "        if word in stop_words or len(word) < 2:\n",
        "            continue\n",
        "\n",
        "        # Remove profane or inappropriate words\n",
        "        if profanity.contains_profanity(word):\n",
        "            continue\n",
        "\n",
        "        cleaned_words.append(word)\n",
        "\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "# === Apply cleaning ===\n",
        "df['cleaned_text'] = df['content'].astype(str).apply(clean_text)\n",
        "\n",
        "# === Save cleaned version ===\n",
        "df.to_csv(\"/content/cleaned_tweets.csv\", index=False)\n",
        "\n",
        "print(\" Cleaned dataset saved as 'cleaned_tweets.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDZYpyZ3YdEP",
        "outputId": "44f67483-1e84-45b0-fb72-73b49696b918"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned dataset saved as 'cleaned_tweets.csv'\n"
          ]
        }
      ]
    }
  ]
}